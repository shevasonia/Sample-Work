{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics Assignment Group 12\n",
    "##### Group Members:\n",
    "| Name | SNR |\n",
    "| :---- | :--- |\n",
    "|Nadya Hagen | 2049115 |\n",
    "| KenÃ©z KovÃ¡cs | 2040678 |\n",
    "| Aryo Bimo Nugroho | 2039696 |\n",
    "| Sheva Sonia Rahmani | 2075109 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a model that aims to predict the direction of the movement of the price of a dogecoin based on Elon Musk's tweets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sheva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading libraries, getting resources:\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data initialization\n",
    "We expect the user of the model to know their dataset well, the code itself does not have data preparative features. In our case the raw data was processed using mostly pandas and excel features, we felt like the code needn't be  included here as it is not remotely reproducible anyways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/sheva/Downloads/data_fluctuations.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the functions that classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the function that makes a list of lists of words of the tweets \n",
    "\n",
    "def into_words(tweetcolumn):\n",
    "    wordlist = []\n",
    "    for tweet_sentence in tweetcolumn:\n",
    "            tweet_sentence_string_lower = str(tweet_sentence).lower()\n",
    "            tweet_words = tweet_sentence_string_lower.split()\n",
    "            wordlist.append(tweet_words)\n",
    "    return(wordlist)\n",
    "\n",
    "# We create the function that removes stopwords from the list, thus creating a stopword-free list of words\n",
    "\n",
    "def filterstopwords(wordlist):\n",
    "    filtered_wordlist = []\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    for lists in wordlist:\n",
    "        for words in lists:\n",
    "            if  words.lower() not in stop_words:\n",
    "                \n",
    "                filtered_wordlist.append(words.lower())\n",
    "    return(filtered_wordlist)\n",
    "\n",
    "# We create the function that makes a list of the top 5000 words used by Elon Musk in his tweetings\n",
    "\n",
    "def top3kwordizer(filtered_wordlist):\n",
    "    all_words = nltk.FreqDist(filtered_wordlist)\n",
    "    all_features = list(all_words)[:4000]\n",
    "    return all_features\n",
    "\n",
    "# We create the function that makes the list of lists that contain dictionaries with the features\n",
    "\n",
    "def list_of_dicts(tweet_sentence, all_features): \n",
    "    tweet_words = set(tweet_sentence)\n",
    "    features = {}\n",
    "    for word in all_features:\n",
    "        if word in tweet_words:\n",
    "            features['contains({})'.format(word)] = True\n",
    "        else:\n",
    "            features['contains({})'.format(word)] = False\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to classified set of data\n",
    "Here we use the functions which we have defined above to go from our original dataset to a classified set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the functions one by one to classify the data\n",
    "wordized_list = into_words(data['tweet']) # we have the list of lists of words\n",
    "filteredwords = filterstopwords(wordized_list) # we have the list of all words filtered free of stopwords\n",
    "top3kwords = top3kwordizer(filteredwords) # we have the top100 words from the list above\n",
    "dictslist = list() # we get the list of list of dictionaries containing the features and if theyre in the tweet or not\n",
    "for tweet in wordized_list: \n",
    "        dictslist.append(list_of_dicts(tweet, top3kwords))\n",
    "        \n",
    "# Using pandas's very nice dataframe feature to build our classified dataset\n",
    "\n",
    "workingdataframe = pd.DataFrame()\n",
    "workingdataframe['features'] = (pd.Series(list(dictslist)))\n",
    "workingdataframe['direction'] = data['direction']\n",
    "\n",
    "# Creating the list of lists that the model will train on\n",
    "\n",
    "finaldata = workingdataframe.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "All that is left to do is to train the model. We first create a training data set to train our model upon and a test data set to test the model's accuracy on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is :  0.5377777777777778\n",
      "Most Informative Features\n",
      "     contains(@tashaark) = True                0 : 1      =     11.3 : 1.0\n",
      "      contains(however,) = True                1 : 0      =      8.4 : 1.0\n",
      "         contains(tests) = True                0 : 1      =      8.3 : 1.0\n",
      "            contains(ðŸ”¥ðŸ”¥) = True                1 : 0      =      7.6 : 1.0\n",
      "contains(@justpaulinelol) = True                0 : 1      =      7.0 : 1.0\n",
      "contains(@teslaratiteam) = True                1 : 0      =      6.9 : 1.0\n",
      "           contains(pcr) = True                0 : 1      =      6.4 : 1.0\n",
      "         contains(turns) = True                0 : 1      =      6.4 : 1.0\n",
      "     contains(@aarons5_) = True                1 : 0      =      6.2 : 1.0\n",
      "contains(@caspar_stanley) = True                0 : 1      =      5.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "train_set = finaldata[450:]\n",
    "test_set = finaldata[:450]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "acc = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy of the model is : \", acc)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/9xn1m47d4j14dpz1gtcsx0wh0000gn/T/ipykernel_4189/4123191309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrefsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mobserved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtestsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from nltk.metrics.scores import (precision, recall)\n",
    "\n",
    "refsets = collections.defaultdict(lambda:train_set)\n",
    "testsets = collections.defaultdict(lambda:test_set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print( 'Precision:', nltk.metrics.precision(refsets['1'], testsets['1']) )\n",
    "print( 'Recall:', nltk.metrics.recall(refsets['1'], testsets['1']) )\n",
    "# `'pos'` is for the \"positive\" (as opposed to \"negative\") label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hope you enjoyed using the model, good luck in life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
